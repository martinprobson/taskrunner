
#
# Default (empty) JDBC config - application.conf should always override this if JDBC connections
# are required...
#
jdbc {
  username = ""
  password = ""
  driver = ""
  url = ""
}
#
# Default (empty) Kerberos config - application.conf should always override this if connecting to
# a kerberos managed resource.
#
kerberos {
  username = ""
  password = ""
  principal = ""
}
#
# spark-python task: Default arguments for the spark submit command.
#
spark-python {
  master = "local[*]"
  queue = "default"
  num-executors = 2
  timeoutms = 600000
}

#
# Default timeout (in milli-seconds) for hive task executor
#
hive {
  timeoutms = 600000
}


jobrunner {
  # plugintasks - These are the list of task types that the Task Runner framework
  #               will handle. Each task is configured by three items: -
  #                   1. The name of the task (String).
  #                   2. The name of the (Guice) plugin module that injects the
  #                      Task into the framework.
  #                   3. The list of file extensions supported by this plugin.
  #
  plugintasks {
    task = [
      {
        name = "dummy"
        plugin-module = "net.martinprobson.jobrunner.dummytask.DummyTaskModule"
        file-extensions = [".txt", ".dmy"]
      }
      {
        name = "jdbc"
        plugin-module = "net.martinprobson.jobrunner.jdbctask.JDBCTaskModule"
        file-extensions = [".sql"]
      }
      {
        name = "hive"
        plugin-module = "net.martinprobson.jobrunner.hivetask.HiveTaskModule"
        file-extensions = [".hql"]
      }
      {
        name = "spark-python"
        plugin-module = "net.martinprobson.jobrunner.sparkpythontask.SparkPythonTaskModule"
        file-extensions = [".py"]
      }

    ]
  }

  #
  # Default number of threads to use when running tasks.
  # You should override this in application.conf according to
  # requirements.
  #
  threads = 2

}
